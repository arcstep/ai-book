{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variable类型与自动微分模块"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variable类型是PyTorch中的另一个变量类型，是Autograd（自动微分）模块对张量进一步封装实现的，支持自动求导。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable对象与Tensor对象之间的转换\n",
    "\n",
    "torch.autograd.Variable是PyTorch的核心类，它用于封装张量，并记录应用于张量上的操作。<br>\n",
    "Variable类主要用于自动梯度计算，即自动求导。\n",
    "\n",
    "在PyTorch 0.4.0及以后的版本中，torch.Tensor和torch.autograd.Variable已经合并，你可以直接在torch.Tensor上进行自动求导。<br>\n",
    "以下是一个使用torch.Tensor进行自动求导的示例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tensor([4.], requires_grad=True)\n",
      "y: tensor([16.], grad_fn=<MulBackward0>)\n",
      "True\n",
      "x.grad: tensor([8.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 创建一个张量，并设置requires_grad=True来跟踪它的计算历史\n",
    "x = torch.tensor([4.0], requires_grad=True)\n",
    "print(\"x:\", x)\n",
    "\n",
    "# 定义一个函数\n",
    "y = x * x\n",
    "print(\"y:\", y)\n",
    "\n",
    "# 计算梯度\n",
    "y.backward()\n",
    "\n",
    "# 打印梯度\n",
    "print(y.requires_grad)\n",
    "print(\"x.grad:\", x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 控制梯度计算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch.no_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y: tensor([16.])\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    # 定义一个函数\n",
    "    y = x * x\n",
    "    print(\"y:\", y)\n",
    "\n",
    "print(y.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### no_grad()装饰器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "z1 = x * 2\n",
    "print(z1.requires_grad)\n",
    "\n",
    "@torch.no_grad()\n",
    "def doubler(x):\n",
    "    return x * 2\n",
    "\n",
    "z2 = doubler(x)\n",
    "print(z2.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### enable_grad() 与 no_grad() 嵌套"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**用函数 enable_grad() 配合 with 嵌套：**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "with torch.no_grad():\n",
    "    # 启用grad\n",
    "    with torch.enable_grad():\n",
    "        y = x * 2\n",
    "print(y.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**enable_grad()装饰器：**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "@torch.enable_grad()\n",
    "def doubler(x):\n",
    "    return x * 2\n",
    "\n",
    "with torch.no_grad():\n",
    "    # 被enable_grad装饰后，不再受no_grad限制\n",
    "    z = doubler(x)\n",
    "print(z.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**enable_grad()函数作用在没有requires_grad属性的Variable对象上时，将无效：**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(2, 2)\n",
    "with torch.enable_grad():\n",
    "    y = x * 2\n",
    "print(y.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set_grad_enabled()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "\n",
    "# 统一关闭梯度计算\n",
    "torch.set_grad_enabled(False)\n",
    "y = x * 2\n",
    "print(y.requires_grad)\n",
    "\n",
    "# 统一打开梯度计算\n",
    "torch.set_grad_enabled(True)\n",
    "y = x * 2\n",
    "print(y.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable对象的属性"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### grad_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(2, 2)\n",
    "print(x.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "m = x + 1\n",
    "print(m.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<AddBackward0 object at 0x11b8d8af0>\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "m = x + 1\n",
    "print(m.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2500, 0.2500],\n",
      "        [0.2500, 0.2500]]) tensor(2., grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "y = x + 1\n",
    "\n",
    "# 修改了grad_fn\n",
    "z = y.mean()\n",
    "z.backward()\n",
    "print(x.grad, z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面代码是关于`grad_fn`的解释：\n",
    "\n",
    "1. x是一个叶子节点，它是由用户直接创建的，所以它的grad_fn是None。\n",
    "2. m是通过加法操作创建的，所以它的grad_fn是一个AddBackward对象，这个对象表示加法操作的反向传播函数。\n",
    "3. f是通过mean操作创建的，所以它的grad_fn是一个MeanBackward对象，这个对象表示mean操作的反向传播函数。\n",
    "4. 当你调用f.backward()时，PyTorch会使用这些grad_fn来构建计算图，然后使用这个计算图来计算梯度。在这个过程中，grad_fn起到了关键的作用。\n",
    "5. 最后，x.grad给出了f对x的梯度。这个梯度是通过计算图和链式法则计算得到的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### is_leaf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在PyTorch中，is_leaf是torch.Tensor的一个属性，它表示这个张量是否是计算图中的叶子节点。\n",
    "\n",
    "在PyTorch的自动求导系统中，所有的计算操作都会形成一个计算图。<br>\n",
    "在这个计算图中，叶子节点通常是用户创建的张量，非叶子节点是通过计算操作生成的张量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "print(x.is_leaf)\n",
    "m = x + 2\n",
    "print(m.is_leaf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**在PyTorch的自动求导系统中，区分叶子节点和非叶子节点有几个重要的作用：**\n",
    "\n",
    "- 内存管理：在反向传播过程中，为了节省内存，PyTorch会自动清除非叶子节点的梯度。这是因为非叶子节点的梯度在计算完成后通常不再需要，所以可以被立即清除。而叶子节点的梯度通常需要保留，因为它们通常对应于模型的参数，我们需要使用它们的梯度来更新参数。\n",
    "\n",
    "- 梯度计算：只有叶子节点的梯度会被自动计算并存储。如果你试图访问非叶子节点的梯度，你会发现它是None。如果你需要保留非叶子节点的梯度，你可以使用retain_grad()函数。\n",
    "\n",
    "- 计算跟踪：只有叶子节点会被计算跟踪。如果你创建一个张量并设置requires_grad=True，那么这个张量就是一个叶子节点，所有对它的操作都会被跟踪，用于后续的梯度计算。而非叶子节点则不会被跟踪。\n",
    "\n",
    "总的来说，区分叶子节点和非叶子节点是PyTorch自动求导系统的一个重要部分，它对内存管理、梯度计算和计算跟踪都有重要作用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### detach() 分离叶子节点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "\n",
    "# 运行后会报错\n",
    "# 必须提前使用 detach() 分离叶子节点\n",
    "x.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1.],\n",
       "       [1., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-chinese-py3.9-ipkyernel",
   "language": "python",
   "name": "langchain-chinese-py3.9-ipkyernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
